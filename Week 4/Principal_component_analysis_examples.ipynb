{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Principal component analysis\n",
    " \n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this train, we learn about dimensionality reduction and choosing principal components to explain the highest variance in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "\n",
    "* Understand dimensionality reduction.\n",
    "* Understand principal component analysis (PCA) as a method for dimensionality reduction. \n",
    "* Implement PCA in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "A widely encountered problem in machine learning is that of dimensionality. We typically refer to the size or shape of a dataset as an $n$ x $p$ matrix, where each row from 1 to $n$ represents an observation or data point, and each column from 1 to $p$ represents a variable or feature. With each additional feature, the dimensionality of a dataset increases by 1.\n",
    "\n",
    "The problems with increasing or high levels of dimensionality are as follows:\n",
    "\n",
    "- More storage space required for the data;\n",
    "- More computation time required to work with the data; and\n",
    "- More features mean more chance of feature correlation, and hence feature redundancy.\n",
    "\n",
    "The latter point is the basis on which **principal component analysis** is carried out. A feature that is highly correlated with another increases when the other increases (positive correlation) or decreases when the other increases (negative correlation). This is helpful because if multiple features tend to behave in a corresponding manner in the dataset, they can often be replaced by some smaller number of representative feature(s). This helps to lower the feature space within which the data reside, reducing computation time as well as storage capacity requirements.\n",
    "\n",
    "The goal of dimensionality reduction is to **reduce the number of features in a dataset while minimising the amount of data loss**. There are three primary methods by which this can be done:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Generalised Discriminant Analysis (GDA)\n",
    "\n",
    "In this notebook, we will focus on **PCA**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "The premise of PCA is that data in some higher number of dimensions can be mapped to some lower number of dimensions whilst retaining the maximum amount of variance in the lower dimension. Broadly speaking, the following steps are involved in a PCA:\n",
    "\n",
    "1. Perform feature scaling on data; \n",
    "2. Construct the covariance matrix of the data;\n",
    "3. Compute the eigenvectors of this matrix. The eigenvectors corresponding to the largest eigenvalues are used to reconstruct a maximal fraction of variance of the original data.\n",
    "\n",
    "These are unfamiliar terms, which we encourage you to investigate on your own. The general idea is that we are able to compute a vector within the feature space of the dataset, which points in the direction of the maximum variance found in the data. This vector is known as the principal component, and we say that it _explains_ the most variance.\n",
    "\n",
    "The second principal component is that vector which is orthogonal (mutually perpendicular) to the first principal component, and which again explains the maximum variance in this orthogonal direction. The third principal component is orthogonal to both of the first two and explains the maximum variance in that direction.\n",
    "\n",
    "This continues for each additional principal component until the maximum number of `p` principal components (there cannot be more principal components than there are features). 100% of the variance can only be explained by all `p` of the features, so we expect to lose some data while reducing the number of dimensions. However, because the principal components are orthogonal and ranked in decreasing order of variance explained, we do normally expect that some subset of all the features will explain a significant proportion of the data.\n",
    "\n",
    "Below is an image which summarises the steps involved in PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "Image(url= \"https://github.com/Explore-AI/Pictures/blob/master/sketch-pca-diagram.png?raw=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA to real data\n",
    "\n",
    "The dataset we will use is from the US census of 2015. It contains demographic information such as population, proportion of race, gender, income, child poverty, etc, for each county in the US. A county is a geographic area which is larger than a city but smaller than a state. There are over 3000 counties in the dataset, which should provide sufficient information to perform PCA with.\n",
    "\n",
    "Let's start this example by importing all the packages we'll need, then bring in the data and see what it looks like. \n",
    "\n",
    "> Note: You will need to install the `cufflinks` and `plotly` libraries to generate the plots in this section. Check out [this guide](https://github.com/plotly/plotly.py#jupyterlab-support-python-35) for more help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cufflinks as cf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='muted',\n",
    "        rc={'figure.figsize': (15,10)})\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/unsupervised_sprint/acs2015_county_data.csv', encoding=\"UTF-8\").dropna()\n",
    "us_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the first few rows of the data and then seeing the shape above, we note that there are 37 columns. Not all of these columns are numeric and some of them, like `ID`, `State` and `County`, could be considered labels. Labels are not used in an unsupervised learning environment - but perhaps they will be of use to us later in validating the clusters in the next train. For now, let's remove those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create short list of unwanted columns\n",
    "labels = ['CensusId', 'State', 'County']\n",
    "\n",
    "# declare the features to be all columns, less the unwanted ones from above\n",
    "features = [col for col in us_df.columns if col not in labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scales of measurement\n",
    "\n",
    "The most important preprocessing step for our data before any dimensionality reduction may take place is **scaling**.\n",
    "\n",
    "Scaling data is vitally important because not all variables are measured on the same scales and/or using the same units. For instance, the age variable will have a range of 0 to approximately 95, with a mean somewhere around 40. Income, however, will be measured over a much larger numeric range, well into the thousands or millions, with a very different mean. Let's take a look at box plots of the first few variables to better understand this measurement discrepancy.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.set_config_file(offline=True, world_readable=True, theme='ggplot')\n",
    "\n",
    "# using plotly to plot the boxplot\n",
    "us_df[features].iplot(kind='box', title=\"Boxplots of Features (Unscaled)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the scales used for percentage-based variables are very different from population-based ones. Let's scale the columns, and plot the boxplots again for the same variables.\n",
    "\n",
    "> Note: If the following cell times out with a warning, check out [this stackoverflow](https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image) for more help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaled data frame variable\n",
    "us_df_scaled = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/unsupervised_sprint/acs2015_county_data.csv', encoding=\"UTF-8\").dropna()\n",
    "us_df_scaled[features] = preprocessing.scale(us_df_scaled[features])\n",
    "\n",
    "# plot boxplots using scaled data\n",
    "us_df_scaled[features].iplot(kind='box', title=\"Boxplots of Features (Scaled)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features now have a mean of zero and a variance of one, but their boxplots remain the same shape.\n",
    "\n",
    "Let's use what we have learned thus far to interpret an important graph. We'll generate a PCA object and fit the features in the scaled data frame to it. We'll then plot a line graph showing the cumulative variance explained versus the increasing number of components. Remember, the only way to retain 100% of the variance is to retain 100% of the components.\n",
    "\n",
    "Conducting PCA using `sklearn` is simple and can be done with just a few lines of code. We first define a PCA object, then fit it to our data and apply the dimensionality reduction. If this was a supervised learning setting and we had a test set, we could simply apply the dimensionality reduction to it using `pca.transform(X_test)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# fit the PCA model to our data and apply the dimensionality reduction \n",
    "prin_comp = pca.fit_transform(us_df_scaled[features])\n",
    "\n",
    "# create a dataframe containing the principal components\n",
    "pca_df = pd.DataFrame(data = prin_comp)\n",
    "\n",
    "pca_df[\"state\"] = us_df[\"State\"]\n",
    "\n",
    "# plot line graph of cumulative variance explained\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a lesson in dimensionality reduction, we'll need to make a compromise: trading variance retained for components kept. Let's set an arbitrary threshold for the desired variance retained of 85% - that is, we want to determine how many principal components will be required to explain 85% of the variance in our dataset. Looking at the graph, it appears roughly 12 components will explain 85% of the data. That is a significant reduction - from 34 features down to 12. Interestingly, it appears that close to 100% of the variance is explained by the first 23 components, itself a significant reduction.\n",
    "\n",
    "Let's check what the actual number of components would be for 85% variance retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_85 = PCA(.85)\n",
    "pca_85.fit_transform(us_df_scaled[features])\n",
    "print(round(pca_85.explained_variance_ratio_.sum()*100, 1),\n",
    "      \"% of variance explained by\",\n",
    "      pca_85.n_components_,\n",
    "      \"components.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, visualising 13 components is not much easier for a human than 34, but the reduction will certainly lower computation time when working with this dataset. Let's take a look at the variance explained by the first few components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, the first component explains 20%, with the 2nd following closely behind at 17% and the 3rd at 13%. Together, the first three components explain 50% of the data, which is quite a significant achievement considering it is less than a tenth of the total number of original components.\n",
    "\n",
    "Let's plot the first two components on a 2D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=pca_df[0], y=pca_df[1],\n",
    "                     hue=\"state\",\n",
    "                     palette='RdBu',\n",
    "                     data=pca_df,\n",
    "                     legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting our dimensionality reduction to use\n",
    "\n",
    "We have seen that just 13 components of an original 34 can be used to explain 86.6% of the variance in our dataset. Let us now utilise our reduced dataset, consisting of just 13 components, by using it to build a predictive regressor model.\n",
    "\n",
    "We will use the feature named `IncomePerCap` as the response variable (the one that we will try to predict). This feature was used in our initial PCA, so we will have to make some changes to the dataset to ensure it is not included this time.\n",
    "\n",
    "PCA requires features to be scaled, as we mentioned above. But we don't need to scale the response variable which we will use for prediction, as it is not used in PCA. This is good, because leaving the response as it is means any predicted values and associated errors are easier to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude non-features from data\n",
    "reg_data = us_df_scaled[features]\n",
    "\n",
    "# set aside response variable (Unscaled!)\n",
    "reg_response = us_df[\"IncomePerCap\"]\n",
    "\n",
    "# drop response variable\n",
    "reg_data = reg_data.drop(['IncomePerCap'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split the dataset up into train and test using an arbitrary ratio of 80/20 (train/test, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reg_data, reg_response, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply PCA to the training set with the number of components set to 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PCA object with n_components set to 13\n",
    "pca_reg = PCA(n_components=13)\n",
    "\n",
    "# fit the PCA model to our data and apply the dimensionality reduction \n",
    "X_train = pca_reg.fit_transform(X_train)\n",
    "\n",
    "# confirm the number of components\n",
    "pca_reg.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reg.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now reduced our dataset of 33 features into just 13 components. Let's continue by applying the same reduction transformation to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pca_reg.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make use of a random forest regression model to do our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, n_jobs=-1, random_state = 101)\n",
    "\n",
    "# train the model on training data\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the trained model to make predictions from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# calculate the absolute errors\n",
    "errors_reg = abs(predictions - y_test)\n",
    "\n",
    "# print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors_reg), 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is able to predict `IncomePerCap` values with a mean absolute error of around $1700. Is that good? Well, let's take a look at the median income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_response.agg(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicts IncomePerCap to within\", round((round(np.mean(errors_reg), 2)/reg_response.agg(\"median\"))*100, 1), \"% of its value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median income value is $23,457, which means our model is able to predict to within ~7% of the actual value on average.\n",
    "\n",
    "We have managed to make predictions using our reduced dataset, but we have no benchmark metric with which to compare our results. We'll need to train a similar model using our original dataset and check its prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and response data from full dataset\n",
    "features = us_df_scaled[features].drop(['IncomePerCap'], axis=1)\n",
    "response = us_df['IncomePerCap']\n",
    "\n",
    "# split original data\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(features, response, test_size=0.2)\n",
    "\n",
    "# instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, n_jobs=-1, random_state = 101)\n",
    "\n",
    "# train the model on training data\n",
    "rf.fit(X_train_orig, y_train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test_orig)\n",
    "\n",
    "# calculate the absolute errors\n",
    "errors = abs(predictions - y_test_orig)\n",
    "\n",
    "# print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicts IncomePerCap to within\", round((round(np.mean(errors), 2)/reg_response.agg(\"median\"))*100, 1), \"% of its value.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the entire dataset, in other words, all 33 features, we are only able to predict `IncomePerCap` to within 6% of its actual value, on average!\n",
    "\n",
    "Thus, it seems using dimensionality reduction here was effective - we reduced the number of features from 33 to 13, and the error in prediction only worsened by 1% (from 6% to 7%). You can imagine that this would be very useful with an extremely large dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and disadvantages of PCA \n",
    "\n",
    "**Advantages**:\n",
    "- Effective at finding optimal representation of a dataset with fewer dimensions\n",
    "- Effective at decreasing redundancy and filtering noise  \n",
    "- Visualisation of datasets with high-dimensionality\n",
    "- Improves performance of algorithm \n",
    "\n",
    "**Disadvantages**:\n",
    "- May result in some loss of information\n",
    "- Variables may become less interpretable after being transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have now learnt what dimensionality reduction means and why one would use it. You have seen how using PCA can be effective in reducing the dimensions and thus the computational complexity when performing regression. Purpose to experiment with the above example to gain a further understanding of PCA and its uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
